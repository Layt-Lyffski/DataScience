{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Componet Analysis (PCA)\n",
    "---\n",
    "prediciton error the distace between point and where it get prdection on line/area\n",
    "##### Reduce from 2-dimension to 1-dimension: \n",
    "Find a direction (= vector $u^{(1)}\\in\\mathbb{R}^n$) onto which to project the data so to minimize the projection error.\n",
    "##### Redcue from n-dimension to k-diemension:\n",
    "Find $k$ vectors $u^{(1)},u^{(2)},...,u^{(k)}$ onto which to project the data, so as to minimize the projection error.<br>\n",
    "__THUS__ try to finde the lowers distance between point and predictive slope\n",
    "<br>\n",
    "__NOTE:_ PCA is not linear regression\n",
    "$\\because$ in linear regression you calcualte the vertical distance between the slope and point, where in PCA you calculate the, indeed, shortest distance between the slope and point (so basically __orthogonal__ to slope) and PCA is not supervised learning, so no search for $y$.\n",
    "<br><br>\n",
    "\n",
    "### Data Preprocessing\n",
    "Trainign set: $x^{(1)},x^{(2)},...,x^{(Â´m)}$<br>\n",
    "Preprocessing (= feature scaling/mean normalization):<br>\n",
    "$\\mu_j = \\frac{1}{m}\\sum_\\limist{i=1}^m x_j^{(i)}$<br>\n",
    "Repace each $x_j^{(i)}$ with $x_j -\\mu_j$<br>\n",
    "<br>\n",
    "If different features on differetn scales (e.g. $x_1 =$ size of house, $x_2 =$ number of bedrooms), scale features to have comparable range of values. (SEE SCALLING ON LINEAR REGRESSION)<br>\n",
    "$x_j^{(i)} \\leftarrow \\frac{x_j^{(i)}-\\mu_j}{s_j}$, MEAN substract the mean, and dived by measuer of the range of values of features j, so it can be wheater the (max - min)j value or the standard deviation<br>\n",
    "__NOTE__ the Mathematical proof of the best \"slope\" is very high mathematic, so here is not included<br>\n",
    "#### Algorithm\n",
    "reduce data from $n$-dimensions to $k$-dimensions Compute \"covariacne matrix\":<br>\n",
    "\n",
    "(just a matrix notation) $\\Sigma = \\frac{1}{m}\\sum_\\limits{i=1}^n\\color{red}{(x^{(i)})(x^{(i)})^T}$<br>\n",
    "\n",
    "**(where $x^{(i)}$ is $n\\times1$ AND $(x^{(i)})^T$ is $1\\times n \\therefore$ IT IS $n\\times n$)<br>\n",
    "\n",
    "Compute \"eigenvectors\" of matric $\\Sigma$ (it is matrix): where result also is $n\\times n$ matrix. <br>\n",
    "__THUS__ the first k-vetors of result-Matirx build the redueced version of the $\\Sigma$ matrix that were given with higher dims\n",
    "##### End example:\n",
    "$x\\in\\mathbb{R}^n --> z\\in\\mathbb{R}^k$<br>\n",
    "\n",
    "$z^{(i)}= \n",
    "\\begin{bmatrix}\n",
    "|&|&|&|\\\\\n",
    "u^{(1)} & u^{(2)} & ... & u^{(k)}\\\\\n",
    "|&|&|&|\\\\\n",
    "\\end{bmatrix}^T$ <br>\n",
    "\n",
    "where is is $n\\times k$<br>\n",
    "\n",
    "$x^{(i)}$ = $\n",
    "\\begin{bmatrix}\n",
    "- & u^{(1)}^T & - \\\\\n",
    "- & w & - \\\\\n",
    "- & w & - \\\\\n",
    "- & u^{(k)}^T & - \\\\\n",
    "\\end{bmatrix}$ <br>\n",
    "(where is is $k\\times n$) * $x^{(i)} \\text{ is a } n \\times 1$<br>\n",
    "__THUS__ $x^{(i)}$ is a $k\\times 1$<br>\n",
    "\n",
    "##### Summary\n",
    "After mean normalization (ensure every features has zero mean) and optionally fearues scaling:\n",
    "implement := Sigma = (1/m) * X^T * X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
