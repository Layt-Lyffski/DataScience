{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "##### Sigmoid/Logcistic function\n",
    "$h_\\theta(x) = g(\\theta^T x) \\rightarrow g(z) = \\frac{1}{1+e^{-z}}\\rightarrow h_\\theta(x) = \\frac{1}{1+e^{-\\theta^T x}}$<br>\n",
    "\n",
    "$\\therefore h_\\theta(x) = P(y=1|x;\\theta) \\therefore$ $0 <= h_\\theta(x) <= 1 $<br>\n",
    "\n",
    "$\\therefore$ probability that $y = 1$, given $x$, parameterized by $\\theta$\n",
    "<br>\n",
    "<br>\n",
    "### Decison Boundary\n",
    "##### Linear Boundary\n",
    "$h_\\theta(x) = g(\\theta_0+\\theta_1 x_1 + \\theta_2 x_2)$\n",
    "\n",
    "Predict $y = 1$ if $-2+x_1+x_1 >= 0$ \n",
    "// you can vary with the predict by combinating with signs e.g $0+x_1+x_2 = 3 \\rightarrow y =0$\n",
    "<br><br>\n",
    "\n",
    "\n",
    "##### Non-linear Boundary\n",
    "$h_\\theta(x)=g(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 +\\theta_3 x1_1m^2 + \\theta_4 x_2^2)$\n",
    "\n",
    "<br>\n",
    "Predict $y = 1$ if $-1 +x_1^2 + x_2^2 >= 0$<br>\n",
    "### Cost function\n",
    "\n",
    "$$Cost_(h_\\theta(x),y)= \\binom{-log(h_\\theta)) if y = 1}{-log(1-h_\\theta(x)) if y = 0}$$\n",
    "\n",
    "$$J(\\theta)=\\frac{1}{m}\\sum_\\limits{i=1}^m Cost(h_\\theta(x^{(i)}), y^{(i)})$$\n",
    "\n",
    "$$J(\\theta)=-\\frac{1}{m} \\sum_\\limits{i=1}^m \\color{green}{y^{(i)} log h_\\theta(x^{(i)})}+\\color{red}{(1-y^{(i)}) log (1-h_\\theta(x)^{(i)}))}$$\n",
    "\n",
    "Fit parametrs $\\theta: \\binom{min}{\\theta} J(\\theta)$<br>\n",
    "Prediction given new $x$:\n",
    "Output $h_\\theta(x) = \\frac{1}{1+e^{\\theta x}}$<br>\n",
    "$\\therefore$ want $min_\\theta J(\\theta)$<br>\n",
    "\n",
    "Repeat { <br>\n",
    "$$\\theta_j := \\theta_j - \\alpha \\color{#2080ac}{\\frac{\\delta}{\\delta \\theta_j}J(\\theta)}$$\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\color{#2080ac}{ \\sum_\\limits{i=1}^m (h_\\theta(x^{(i)}), y^{(i)}) x_j^{(i)}}$$\n",
    "\n",
    "} (simultaneously update all $\\theta_j$) //NOTE: altough the Algorihm loook id. with liear Regression, DENOTE: that the $h_\\theta$ (Hypothesis) change<br>\n",
    "\n",
    "__Converging__ work the same as in Linear Regression\n",
    "<br>\n",
    "#### Advanced Optimizaiton not need for learning rate alpha\n",
    "• Conjugate gradient<br>\n",
    "• BFGS<br>\n",
    "• L-BFGS<br>\n",
    "\n",
    "## Mulitclass Classification\n",
    "##### One-vs-all/rest\n",
    "$$h_\\theta^{(i)}(x) = P(y=i|x;\\theta) WHERE (i=1,2,3)$$\n",
    "\n",
    "Train a logistic regression classifire $h_\\theta^{(i)}$ for each class $i$ to predict the probability that $y = i$<br>\n",
    "\n",
    "On a new input $x$, to make a prediction, pic the class $i$ that maximizes $\\binom{max}{i}h_\\theta^{(i)}(x)$\n",
    "\n",
    "<images src=\"pic/one-vs-all.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
